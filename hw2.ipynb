{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement train\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Train method not implemented\")\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement inference\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Inference method not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "def conlleval(p, g, w, filename='tempfile.txt'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, ww in zip(sl, sp, sw):\n",
    "            out += ww + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename, 'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "\n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain precision/recall and F1 score '''\n",
    "    _conlleval = 'conlleval.pl'\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename).read())\n",
    "    for line in stdout.split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "\n",
    "    precision = float(out[6][:-2])\n",
    "    recall    = float(out[8][:-2])\n",
    "    f1score   = float(out[10])\n",
    "\n",
    "    return (precision, recall, f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_target(label, size):\n",
    "#     # print \"label is \", label\n",
    "#     tensor = torch.zeros(size)\n",
    "#     tensor = tensor.long()\n",
    "#     tensor[label] = 1\n",
    "#     # print \"tensor is \", tensor\n",
    "#     return tensor.view(1,-1)\n",
    "#     # return torch.LongTensor([label.tolist()])\n",
    "\n",
    "\n",
    "class MyNNClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def inference(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, output_dimension):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_linear = nn.Linear(num_input_nodes, num_hidden_nodes)\n",
    "        self.output_linear = nn.Linear(num_hidden_nodes, output_dimension)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        out = self.input_linear(input_vector)\n",
    "        out = F.tanh(out)\n",
    "        out = self.output_linear(out)\n",
    "        out = F.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(word_embeddings, prev_label, word):\n",
    "    word_embedding = word_embeddings[word]\n",
    "    word_feature = autograd.Variable(word_embeddings[word])\n",
    "    prev_label = autograd.Variable(prev_label)\n",
    "#     input_vector = torch.cat((word_embedding.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "    input_vector = torch.cat((word_feature.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "#     print \"input vector \", input_vector\n",
    "    return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train data  3983   3983\n"
     ]
    }
   ],
   "source": [
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument(\"--data\", type=str, default=\"atis.small.pkl.gz\", help=\"The zipped dataset\")\n",
    "\n",
    "# parsed_args = argparser.parse_args(sys.argv[1:])\n",
    "\n",
    "filename = \"atis.small.pkl.gz\"\n",
    "f = gzip.open(filename,'rb')\n",
    "train_set, valid_set, test_set, dicts = cPickle.load(f)\n",
    "\n",
    "# print \"train_set \", train_set\n",
    "\n",
    "train_lex, _, train_y = train_set\n",
    "valid_lex, _, valid_y = valid_set\n",
    "test_lex,  _,  test_y  = test_set\n",
    "\n",
    "# print \"train_lex \", train_lex\n",
    "# print \"train_y \", train_y\n",
    "\n",
    "idx2label = dict((k,v) for v,k in dicts['labels2idx'].iteritems())\n",
    "idx2word  = dict((k,v) for v,k in dicts['words2idx'].iteritems())\n",
    "\n",
    "'''\n",
    "To have a look what the original data look like, commnet them before your submission\n",
    "'''\n",
    "print \"length train data \", len(train_lex), \" \", len(train_y)\n",
    "# print \"test lex \", test_lex[0]\n",
    "# print \"word dictionary is \", idx2word\n",
    "# print \"label dictionary is \", idx2label\n",
    "# print train_lex[0], map(lambda t: idx2word[t], train_lex[0])\n",
    "# print train_y[0], map(lambda t: idx2label[t], train_y[0])\n",
    "# print test_lex[0], map(lambda t: idx2word[t], test_lex[0])\n",
    "# print test_y[0], map(lambda t: idx2label[t], test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(train_x, train_y):\n",
    "    NUM_LABELS = len(idx2label)\n",
    "    VOCAB_SIZE = len(idx2word)\n",
    "    word_embedding_list = []\n",
    "    label_list = []\n",
    "    \n",
    "#     word_embeddings = torch.rand(VOCAB_SIZE, 300)\n",
    "    word_embeddings = torch.eye(VOCAB_SIZE, VOCAB_SIZE)\n",
    "    print \"VOCAB SIZE\", VOCAB_SIZE\n",
    "    print \"NUM LABELS \", NUM_LABELS\n",
    "    tag_embeddings = torch.rand(NUM_LABELS+1, 100)\n",
    "#     tag_embeddings = torch.eye(NUM_LABELS+1, NUM_LABELS+1)\n",
    "    for sentence, labels in zip(train_x, train_y):\n",
    "        prev_label = tag_embeddings[NUM_LABELS]\n",
    "        for word, label in zip(sentence, labels):\n",
    "            word_embedding = word_embeddings[word]\n",
    "#             word_feature = autograd.Variable(word_embedding)\n",
    "#             prev_label = autograd.Variable(prev_label)\n",
    "            input_vector = torch.cat((word_embedding.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "#             print \"input vector \", input_vector\n",
    "#             input_vector = autograd.Variable(input_vector)\n",
    "#             input_vector = torch.cat((word_feature, prev_label), 1)\n",
    "            word_embedding_list.append(input_vector)\n",
    "#             print \"word embedding list \", word_embedding_list\n",
    "            prev_label = tag_embeddings[label]\n",
    "            # input_vector = autograd.Variable(concat_vec)\n",
    "            # print \"input vector \", input_vector\n",
    "            label_tensor = torch.LongTensor(NUM_LABELS).zero_().view(1,-1)\n",
    "            label_tensor[0,label] = 1\n",
    "#             label_tensor = torch.LongTensor([label.item()])\n",
    "#             target = autograd.Variable(label_tensor)\n",
    "            label_list.append(label_tensor)\n",
    "#             print \"label list \", label_list\n",
    "    print \"word embedding list \", len(word_embedding_list[0])\n",
    "    print \"label list \", len(label_list)\n",
    "#     print \"label list 0 \", label_list[0]\n",
    "    return word_embedding_list, label_list\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE 572\n",
      "NUM LABELS  127\n",
      "word embedding list  1\n",
      "label list  45388\n",
      "word_embeddings  torch.Size([45388, 672])\n",
      "number of input nodes  672\n",
      "label list  torch.Size([45388, 127])\n"
     ]
    }
   ],
   "source": [
    "word_embedding_list, label_list = create_embedding(train_lex, train_y)\n",
    "\n",
    "'''\n",
    "implement you training loop here\n",
    "'''\n",
    "# NUM_LABELS = len(idx2label)\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "HIDDEN_NODES = 200\n",
    "NUM_LABELS = len(idx2label)\n",
    "# word_embeddings = torch.rand(VOCAB_SIZE, 100)\n",
    "# word_embeddings = torch.eye(VOCAB_SIZE, VOCAB_SIZE)\n",
    "# tag_embeddings = torch.rand(NUM_LABELS+1, 100)\n",
    "# tag_embeddings = torch.eye(NUM_LABELS+1, NUM_LABELS+1)\n",
    "word_embedding_list = torch.stack(word_embedding_list)\n",
    "word_embedding_list = torch.squeeze(word_embedding_list)\n",
    "print \"word_embeddings \", word_embedding_list.size()\n",
    "label_list = torch.stack(label_list)\n",
    "label_list = torch.squeeze(label_list)\n",
    "label_list = label_list.float()\n",
    "NUM_INPUT_NODES = word_embedding_list[0].size()[0]\n",
    "print \"number of input nodes \", NUM_INPUT_NODES\n",
    "print \"label list \", label_list.size()\n",
    "\n",
    "# word_tensors = torch.split(word_embedding_list, 1000)\n",
    "# print \"word tensors\", word_tensors\n",
    "# label_tensors = torch.split(label_list, 1000)\n",
    "\n",
    "# print \"tag_embeddings \", tag_embeddings\n",
    "# input dimension for neural network is concatenation of word and tag tensors\n",
    "model = NeuralNet(NUM_INPUT_NODES, HIDDEN_NODES, NUM_LABELS)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=10\n",
    "#                       , momentum=0.9\n",
    "                     )\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "words = autograd.Variable(word_embedding_list\n",
    "#                           , requires_grad=True\n",
    "                         )\n",
    "label = autograd.Variable(label_list\n",
    "                          , requires_grad=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  0.00774754025042\n",
      "loss  0.00774643756449\n",
      "loss  0.00774517375976\n",
      "loss  0.00774419074878\n",
      "loss  0.00774312065914\n",
      "loss  0.00774190714583\n",
      "loss  0.00774078210816\n",
      "loss  0.00773952854797\n",
      "loss  0.0077384756878\n",
      "loss  0.00773747544736\n",
      "loss  0.00773683516309\n",
      "loss  0.00773592665792\n",
      "loss  0.00773502979428\n",
      "loss  0.00773431034759\n",
      "loss  0.00773344608024\n",
      "loss  0.00773232616484\n",
      "loss  0.0077311550267\n",
      "loss  0.00772976921871\n",
      "loss  0.007728475146\n",
      "loss  0.00772688863799\n",
      "loss  0.00772527884692\n",
      "loss  0.00772372167557\n",
      "loss  0.0077223428525\n",
      "loss  0.00772078335285\n",
      "loss  0.00771889835596\n",
      "loss  0.00771764526144\n",
      "loss  0.00771667761728\n",
      "loss  0.00771585805342\n",
      "loss  0.00771408621222\n",
      "loss  0.00771243777126\n",
      "loss  0.00771073903888\n",
      "loss  0.00770833203569\n",
      "loss  0.00770640140399\n",
      "loss  0.00770426075906\n",
      "loss  0.00770216668025\n",
      "loss  0.00770023139194\n",
      "loss  0.00769871613011\n",
      "loss  0.00769698712975\n",
      "loss  0.00769480830058\n",
      "loss  0.00769237568602\n",
      "loss  0.00768931955099\n",
      "loss  0.00768623454496\n",
      "loss  0.00768373673782\n",
      "loss  0.00768133159727\n",
      "loss  0.0076792105101\n",
      "loss  0.0076763718389\n",
      "loss  0.00767341069877\n",
      "loss  0.00766942324117\n",
      "loss  0.00766538036987\n",
      "loss  0.00766242435202\n",
      "loss  0.00765967229381\n",
      "loss  0.00765616400167\n",
      "loss  0.00765134301037\n",
      "loss  0.00764577789232\n",
      "loss  0.0076424269937\n",
      "loss  0.00763864209875\n",
      "loss  0.00763286370784\n",
      "loss  0.0076262280345\n",
      "loss  0.00762195885181\n",
      "loss  0.00761614833027\n",
      "loss  0.0076088369824\n",
      "loss  0.00760252820328\n",
      "loss  0.00759493093938\n",
      "loss  0.00758686568588\n",
      "loss  0.00757856853306\n",
      "loss  0.00756897078827\n",
      "loss  0.00755912717432\n",
      "loss  0.00754787120968\n",
      "loss  0.00753593584523\n",
      "loss  0.00752404890954\n",
      "loss  0.00750875566155\n",
      "loss  0.00749233365059\n",
      "loss  0.00747438007966\n",
      "loss  0.0074534281157\n",
      "loss  0.00742995459586\n",
      "loss  0.00740425428376\n",
      "loss  0.00737272622064\n",
      "loss  0.00733675714582\n",
      "loss  0.00729426508769\n",
      "loss  0.00724146375433\n",
      "loss  0.00717864045873\n",
      "loss  0.00710150226951\n",
      "loss  0.00700281793252\n",
      "loss  0.00687575154006\n",
      "loss  0.00670912396163\n",
      "loss  0.00648689642549\n",
      "loss  0.00619100546464\n",
      "loss  0.00581638934091\n",
      "loss  0.00539998710155\n",
      "loss  0.005041629076\n",
      "loss  0.00482667796314\n",
      "loss  0.00473061855882\n",
      "loss  0.00469308067113\n",
      "loss  0.00467779161409\n",
      "loss  0.00466983672231\n",
      "loss  0.00466578360647\n",
      "loss  0.00466228276491\n",
      "loss  0.00465965038165\n",
      "loss  0.00465669296682\n",
      "loss  0.00465435069054\n",
      "loss  0.00465171085671\n",
      "loss  0.00464943284169\n",
      "loss  0.00464767543599\n",
      "loss  0.00464554456994\n",
      "loss  0.00464331591502\n",
      "loss  0.004640951287\n",
      "loss  0.00463918503374\n",
      "loss  0.00463718641549\n",
      "loss  0.00463514681906\n",
      "loss  0.00463318312541\n",
      "loss  0.00463142385706\n",
      "loss  0.00462967948988\n",
      "loss  0.00462749926373\n",
      "loss  0.00462571764365\n",
      "loss  0.00462393229827\n",
      "loss  0.00462234206498\n",
      "loss  0.00462073925883\n",
      "loss  0.0046192323789\n",
      "loss  0.00461751176044\n",
      "loss  0.00461554853246\n",
      "loss  0.0046141911298\n",
      "loss  0.00461232475936\n",
      "loss  0.00461117830127\n",
      "loss  0.00460966071114\n",
      "loss  0.00460825767368\n",
      "loss  0.00460656452924\n",
      "loss  0.00460525834933\n",
      "loss  0.00460388232023\n",
      "loss  0.00460213376209\n",
      "loss  0.00460150884464\n",
      "loss  0.00459966855124\n",
      "loss  0.00459873909131\n",
      "loss  0.00459726434201\n",
      "loss  0.00459550879896\n",
      "loss  0.00459445267916\n",
      "loss  0.00459286337718\n",
      "loss  0.00459189154208\n",
      "loss  0.00459064822644\n",
      "loss  0.00458983844146\n",
      "loss  0.00458854902536\n",
      "loss  0.00458736810833\n",
      "loss  0.00458585517481\n",
      "loss  0.00458437809721\n",
      "loss  0.00458331406116\n",
      "loss  0.00458234502003\n",
      "loss  0.00458086933941\n",
      "loss  0.00457985512912\n",
      "loss  0.00457890890539\n",
      "loss  0.0045776530169\n",
      "loss  0.00457653636113\n",
      "loss  0.00457530701533\n",
      "loss  0.0045743775554\n",
      "loss  0.0045729437843\n",
      "loss  0.00457229325548\n",
      "loss  0.00457127019763\n",
      "loss  0.00456992676482\n",
      "loss  0.0045690368861\n",
      "loss  0.00456818193197\n",
      "loss  0.00456684594974\n",
      "loss  0.00456597423181\n",
      "loss  0.00456482311711\n",
      "loss  0.0045637707226\n",
      "loss  0.00456274021417\n",
      "loss  0.00456168269739\n",
      "loss  0.00456072390079\n",
      "loss  0.00455977907404\n",
      "loss  0.00455865310505\n",
      "loss  0.00455741165206\n",
      "loss  0.00455651944503\n",
      "loss  0.00455533899367\n",
      "loss  0.00455459859222\n",
      "loss  0.00455369474366\n",
      "loss  0.00455262558535\n",
      "loss  0.0045513426885\n",
      "loss  0.00455040996894\n",
      "loss  0.00454941391945\n",
      "loss  0.00454832147807\n",
      "loss  0.00454732123762\n",
      "loss  0.00454628793523\n",
      "loss  0.00454567233101\n",
      "loss  0.00454443600029\n",
      "loss  0.00454375520349\n",
      "loss  0.00454247323796\n",
      "loss  0.00454151537269\n",
      "loss  0.00454059382901\n",
      "loss  0.00453994283453\n",
      "loss  0.00453899707645\n",
      "loss  0.00453790929168\n",
      "loss  0.00453683407977\n",
      "loss  0.00453609228134\n",
      "loss  0.00453522382304\n",
      "loss  0.00453407224268\n",
      "loss  0.0045332396403\n",
      "loss  0.00453244103119\n",
      "loss  0.00453166943043\n",
      "loss  0.00453091459349\n",
      "loss  0.00452933460474\n",
      "loss  0.00452842283994\n",
      "loss  0.00452771317214\n",
      "loss  0.00452663563192\n",
      "loss  0.0045256013982\n",
      "loss  0.0045248279348\n",
      "loss  0.00452401768416\n",
      "loss  0.00452320789918\n",
      "loss  0.0045223091729\n",
      "loss  0.00452103558928\n",
      "loss  0.00452010286972\n",
      "loss  0.00451934384182\n",
      "loss  0.00451890192926\n",
      "loss  0.00451802555472\n",
      "loss  0.00451701926067\n",
      "loss  0.00451606931165\n",
      "loss  0.00451524462551\n",
      "loss  0.00451433751732\n",
      "loss  0.00451342016459\n",
      "loss  0.00451248697937\n",
      "loss  0.00451151980087\n",
      "loss  0.00451085204259\n",
      "loss  0.00451018800959\n",
      "loss  0.00450906949118\n",
      "loss  0.00450821500272\n",
      "loss  0.00450698519126\n",
      "loss  0.00450614560395\n",
      "loss  0.00450523896143\n",
      "loss  0.00450449343771\n",
      "loss  0.00450361100957\n",
      "loss  0.00450249947608\n",
      "loss  0.00450184056535\n",
      "loss  0.00450117979199\n",
      "loss  0.00450040027499\n",
      "loss  0.0044995569624\n",
      "loss  0.00449883006513\n",
      "loss  0.00449757650495\n",
      "loss  0.00449673319235\n",
      "loss  0.00449596438557\n",
      "loss  0.00449528871104\n",
      "loss  0.00449422001839\n",
      "loss  0.00449354248121\n",
      "loss  0.00449246959761\n",
      "loss  0.00449183676392\n",
      "loss  0.00449094595388\n",
      "loss  0.00449022185057\n",
      "loss  0.0044893855229\n",
      "loss  0.00448847329244\n",
      "loss  0.00448796804994\n",
      "loss  0.00448675453663\n",
      "loss  0.00448622135445\n",
      "loss  0.00448514986783\n",
      "loss  0.00448445416987\n",
      "loss  0.00448370818049\n",
      "loss  0.00448297616094\n",
      "loss  0.00448181433603\n",
      "loss  0.00448097335175\n",
      "loss  0.00448002014309\n",
      "loss  0.0044793440029\n",
      "loss  0.00447839125991\n",
      "loss  0.00447765318677\n",
      "loss  0.00447670696303\n",
      "loss  0.0044761528261\n",
      "loss  0.0044751050882\n",
      "loss  0.00447450159118\n",
      "loss  0.0044733802788\n",
      "loss  0.00447279633954\n",
      "loss  0.00447181239724\n",
      "loss  0.00447116885334\n",
      "loss  0.0044701253064\n",
      "loss  0.00446923216805\n",
      "loss  0.00446853740141\n",
      "loss  0.00446783192456\n",
      "loss  0.0044670603238\n",
      "loss  0.00446632551029\n",
      "loss  0.00446541840211\n",
      "loss  0.00446456670761\n",
      "loss  0.0044635431841\n",
      "loss  0.00446249684319\n",
      "loss  0.00446183606982\n",
      "loss  0.00446088472381\n",
      "loss  0.00446003628895\n",
      "loss  0.00445916224271\n",
      "loss  0.00445833988488\n",
      "loss  0.00445772521198\n",
      "loss  0.00445678504184\n",
      "loss  0.00445552775636\n",
      "loss  0.00445498526096\n",
      "loss  0.00445414707065\n",
      "loss  0.00445343274623\n",
      "loss  0.00445260154083\n",
      "loss  0.00445176800713\n",
      "loss  0.00445096474141\n",
      "loss  0.00445026438683\n",
      "loss  0.0044489344582\n",
      "loss  0.00444822059944\n",
      "loss  0.00444726087153\n",
      "loss  0.0044462820515\n",
      "loss  0.00444554025307\n",
      "loss  0.0044448710978\n",
      "loss  0.00444429041818\n",
      "loss  0.00444347178563\n",
      "loss  0.00444264430553\n",
      "loss  0.00444140145555\n",
      "loss  0.00444078724831\n",
      "loss  0.00444003054872\n",
      "loss  0.0044388435781\n",
      "loss  0.00443826522678\n",
      "loss  0.00443755509332\n",
      "loss  0.00443667266518\n",
      "loss  0.00443576136604\n",
      "loss  0.00443522492424\n",
      "loss  0.00443417718634\n",
      "loss  0.00443338416517\n",
      "loss  0.00443234480917\n",
      "loss  0.00443155691028\n",
      "loss  0.0044308565557\n",
      "loss  0.00443012872711\n",
      "loss  0.00442903628573\n",
      "loss  0.00442812265828\n",
      "loss  0.00442749727517\n",
      "loss  0.00442666048184\n",
      "loss  0.00442564208061\n",
      "loss  0.00442496035248\n",
      "loss  0.00442418642342\n",
      "loss  0.00442349445075\n",
      "loss  0.00442273402587\n",
      "loss  0.00442189443856\n",
      "loss  0.00442092679441\n",
      "loss  0.00442014215514\n",
      "loss  0.00441905716434\n",
      "loss  0.00441825622693\n",
      "loss  0.00441706785932\n",
      "loss  0.00441610766575\n",
      "loss  0.00441526528448\n",
      "loss  0.00441455980763\n",
      "loss  0.00441380962729\n",
      "loss  0.00441308924928\n",
      "loss  0.00441229809076\n",
      "loss  0.00441162614152\n",
      "loss  0.00441087409854\n",
      "loss  0.00440991809592\n",
      "loss  0.00440883729607\n",
      "loss  0.00440814811736\n",
      "loss  0.00440737279132\n",
      "loss  0.00440639443696\n",
      "loss  0.00440566008911\n",
      "loss  0.00440510176122\n",
      "loss  0.00440425053239\n",
      "loss  0.00440342212096\n",
      "loss  0.0044025327079\n",
      "loss  0.00440179975703\n",
      "loss  0.00440098997205\n",
      "loss  0.00439938856289\n",
      "loss  0.00439889868721\n",
      "loss  0.00439796550199\n",
      "loss  0.00439704675227\n",
      "loss  0.00439642416313\n",
      "loss  0.00439570657909\n",
      "loss  0.00439503183588\n",
      "loss  0.00439424812794\n",
      "loss  0.00439329538494\n",
      "loss  0.00439245440066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  0.00439176149666\n",
      "loss  0.00439088186249\n",
      "loss  0.00438953051344\n",
      "loss  0.00438883760944\n",
      "loss  0.00438808882609\n",
      "loss  0.00438734097406\n",
      "loss  0.00438638171181\n",
      "loss  0.00438565481454\n",
      "loss  0.00438490649685\n",
      "loss  0.00438405899331\n",
      "loss  0.00438303966075\n",
      "loss  0.0043822741136\n",
      "loss  0.00438109366223\n",
      "loss  0.00438047619537\n",
      "loss  0.00437947409227\n",
      "loss  0.00437862146646\n",
      "loss  0.00437778467312\n",
      "loss  0.00437683379278\n",
      "loss  0.00437597604468\n",
      "loss  0.00437519047409\n",
      "loss  0.00437428522855\n",
      "loss  0.00437346147373\n",
      "loss  0.00437270011753\n",
      "loss  0.0043716263026\n",
      "loss  0.00437100930139\n",
      "loss  0.00437008496374\n",
      "loss  0.00436931988224\n",
      "loss  0.0043684002012\n",
      "loss  0.00436744280159\n",
      "loss  0.0043667005375\n",
      "loss  0.00436577526852\n",
      "loss  0.00436502089724\n",
      "loss  0.00436427351087\n",
      "loss  0.00436362391338\n",
      "loss  0.00436232611537\n",
      "loss  0.00436158338562\n",
      "loss  0.00436073401943\n",
      "loss  0.00435971328989\n",
      "loss  0.00435877032578\n",
      "loss  0.00435806857422\n",
      "loss  0.00435734866187\n",
      "loss  0.00435635540634\n",
      "loss  0.00435558147728\n",
      "loss  0.00435455981642\n",
      "loss  0.00435372488573\n",
      "loss  0.00435248250142\n",
      "loss  0.00435142265633\n",
      "loss  0.00435069855303\n",
      "loss  0.00434995023534\n",
      "loss  0.00434913020581\n",
      "loss  0.0043481211178\n",
      "loss  0.00434737978503\n",
      "loss  0.00434657512233\n",
      "loss  0.00434568477795\n",
      "loss  0.0043449648656\n",
      "loss  0.00434411410242\n",
      "loss  0.00434289919212\n",
      "loss  0.00434206891805\n",
      "loss  0.00434110872447\n",
      "loss  0.00434039905667\n",
      "loss  0.00433937413618\n",
      "loss  0.00433851080015\n",
      "loss  0.00433783279732\n",
      "loss  0.0043368851766\n",
      "loss  0.00433580763638\n",
      "loss  0.00433507980779\n",
      "loss  0.00433402974159\n",
      "loss  0.00433346629143\n",
      "loss  0.0043323864229\n",
      "loss  0.00433168374002\n",
      "loss  0.00433067884296\n",
      "loss  0.0043299458921\n",
      "loss  0.00432882132009\n",
      "loss  0.00432792678475\n",
      "loss  0.00432699499652\n",
      "loss  0.00432593375444\n",
      "loss  0.00432513002306\n",
      "loss  0.00432387832552\n",
      "loss  0.0043230666779\n",
      "loss  0.00432184431702\n",
      "loss  0.00432101637125\n",
      "loss  0.00432019587606\n",
      "loss  0.00431947084144\n",
      "loss  0.00431867223233\n",
      "loss  0.00431776838377\n",
      "loss  0.00431684404612\n",
      "loss  0.00431609526277\n",
      "loss  0.0043149869889\n",
      "loss  0.0043141101487\n",
      "loss  0.00431320583448\n",
      "loss  0.00431186240166\n",
      "loss  0.00431085284799\n",
      "loss  0.00431027216837\n",
      "loss  0.00430915178731\n",
      "loss  0.00430821627378\n",
      "loss  0.00430725328624\n",
      "loss  0.00430641742423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a4ee1e9c890f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             label = autograd.Variable(label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#             print \"labels \", label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m#         if epoch % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#             print \"probs \", probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ff002f6f17e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_vector)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_autograd_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/pointwise.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, inplace)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "#             words = autograd.Variable(word_embedding_list\n",
    "#                           , requires_grad=True\n",
    "#                          )\n",
    "#             label = autograd.Variable(label_list\n",
    "# #                           , requires_grad=False\n",
    "#                          )\n",
    "#         for words, label in zip(word_tensors, label_tensors):\n",
    "    #         model.zero_grad()\n",
    "    #         if epoch % 100 == 0:\n",
    "    #             print \"word embedding list \", word_embedding_list\n",
    "    #         words = autograd.Variable(word_embedding_list.view(-1, NUM_INPUT_NODES))\n",
    "#             words = autograd.Variable(words)\n",
    "#             print \"words \", words\n",
    "#             label = autograd.Variable(label)\n",
    "#             print \"labels \", label\n",
    "            probs = model(words)\n",
    "    #         if epoch % 100 == 0:\n",
    "    #             print \"probs \", probs\n",
    "    # # #             print \"target \", target\n",
    "    #             print \"label list \", label_list\n",
    "            loss = loss_function(probs, label)\n",
    "    #         if epoch % 100 == 0:\n",
    "    #             print loss\n",
    "            print \"loss \", loss.data[0]\n",
    "    #         print \"gradients \", words.grad\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             print \"loss \", loss\n",
    "#     print \"epoch number \", epoch, \" epoch_loss \", epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear (672 -> 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "    output_labels = np.zeros(len(sentence))\n",
    "    prev_label = tag_embeddings[NUM_LABELS]\n",
    "    prob_list = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        input_vector =  get_input_vector(word_embeddings, prev_label, word)\n",
    "        probs = model(input_vector)\n",
    "#         print \"probs greedy\", probs\n",
    "        prob_list.append(probs)\n",
    "        max_val, predicted_label = torch.max(probs, 1)\n",
    "        predicted_label = predicted_label.data[0]\n",
    "        prev_label = tag_embeddings[predicted_label]\n",
    "        output_labels[i] = predicted_label\n",
    "#     print prob_list\n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "        dp = np.zeros((NUM_LABELS, len(sentence)+1))\n",
    "        back_pointers = np.zeros((NUM_LABELS, len(sentence)))\n",
    "        dp[0][0] = 1\n",
    "        for i in range(len(sentence)):\n",
    "            word_table = np.zeros((NUM_LABELS, NUM_LABELS))\n",
    "            if i == 0:\n",
    "                input_vector = get_input_vector(word_embeddings, tag_embeddings[NUM_LABELS], sentence[i])\n",
    "#                 print \"input_vector \", input_vector\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "#                 print \"probs is \", probs\n",
    "#                 word_table[:,0] = np.log(dp[0, 0]) + np.log(probs)\n",
    "#                 word_table[:,0] = dp[0,0] + probs\n",
    "                word_table[:,0] = np.multiply(dp[0, 0], probs)\n",
    "#                 print \"word table 0\", word_table[:, 0]\n",
    "                dp[:,i+1] = word_table[:,0]\n",
    "                back_pointers[:,i] = 128\n",
    "                continue\n",
    "#             print i\n",
    "            for j in range(NUM_LABELS):\n",
    "                input_vector = get_input_vector(word_embeddings, tag_embeddings[j], sentence[i])\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "#                 print \" probs \", probs\n",
    "#                 print \"dp array \", dp[:,i]\n",
    "#                 print np.multiply(dp[:, i], probs)\n",
    "#                 word_table[:,j] = dp[:, i] + probs\n",
    "                word_table[:,j] = np.multiply(dp[:, i], probs)\n",
    "#             print \"word table is \", word_table\n",
    "            dp[:,i+1] = word_table.max(1)\n",
    "            for k in range(NUM_LABELS):\n",
    "                for index, element in enumerate(word_table[k]):\n",
    "                    if element == dp[k, i+1]:\n",
    "                        back_pointers[k, i] = index\n",
    "#                 back_pointers[k, i] = word_table[k].index(dp[k,i+1])\n",
    "\n",
    "#         print \"back_pointers \", back_pointers\n",
    "#         print \"dp matrix \", dp[:,19]\n",
    "        output_labels = np.zeros(len(sentence), dtype = np.int)\n",
    "        label_index = len(sentence) - 1\n",
    "        max_val = dp[:, len(sentence)].max()\n",
    "        for index, element in enumerate(dp[:, len(sentence)]):\n",
    "            if element == max_val:\n",
    "                output_labels[label_index] = index\n",
    "#                 print \"debug2 \", index\n",
    "                break\n",
    "#         print \"output labels \", output_labels\n",
    "        for i in range(len(sentence)-1, 1, -1):\n",
    "#             print \"debug \", output_labels[label_index]\n",
    "            row = back_pointers[output_labels[label_index], i]\n",
    "            label_index -= 1\n",
    "            output_labels[label_index] = row\n",
    "        print \"output labels \", output_labels\n",
    "        return output_labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 428], m2: [672 x 200] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1293",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9cf55eba19c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                               \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                         torch.eye(NUM_LABELS+1, NUM_LABELS+1), NUM_LABELS)) \n\u001b[0;32m----> 7\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_lex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                    ]\n",
      "\u001b[0;32m<ipython-input-23-2db070324fb5>\u001b[0m in \u001b[0;36mgreedy_inference\u001b[0;34m(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minput_vector\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_input_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#         print \"probs greedy\", probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprob_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ff002f6f17e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_vector)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/linear.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 428], m2: [672 x 200] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1293"
     ]
    }
   ],
   "source": [
    "predictions_test = [ map(lambda t: idx2label[t], \n",
    "                             greedy_inference(model, x, \n",
    "#                                               torch.eye(len(idx2word), len(idx2word))\n",
    "                                              torch.rand(VOCAB_SIZE, 300)\n",
    "                                              ,\n",
    "                                        torch.eye(NUM_LABELS+1, NUM_LABELS+1), NUM_LABELS)) \n",
    "                        for x in test_lex\n",
    "                   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions  ['B-flight_days', 'B-transport_type', 'B-arrive_time.time', 'B-flight_days', 'B-transport_type', 'B-flight_days', 'B-flight_days', 'B-flight_days', 'B-transport_type', 'B-flight_days', 'B-flight_days', 'B-transport_type', 'B-arrive_time.time', 'I-arrive_time.end_time', 'I-cost_relative', 'B-depart_time.end_time', 'B-return_date.date_relative', 'B-fromloc.state_name', 'I-depart_date.today_relative']\n"
     ]
    }
   ],
   "source": [
    "print \"predictions \", predictions_test[0]\n",
    "groundtruth_test = [ map(lambda t: idx2label[t], y) for y in test_y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'B-stoploc.city_name', 'I-stoploc.city_name']\n"
     ]
    }
   ],
   "source": [
    "print \"groundtruth \", groundtruth_test[0]\n",
    "words_test = [ map(lambda t: idx2word[t], w) for w in test_lex ]\n",
    "test_precision, test_recall, test_f1score = conlleval(predictions_test, groundtruth_test, words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.04 0.02\n"
     ]
    }
   ],
   "source": [
    "print test_precision, test_recall, test_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
