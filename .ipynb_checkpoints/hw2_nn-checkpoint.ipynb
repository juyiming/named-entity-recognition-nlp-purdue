{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement train\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Train method not implemented\")\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement inference\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Inference method not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "def conlleval(p, g, w, filename='tempfile.txt'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, ww in zip(sl, sp, sw):\n",
    "            out += ww + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename, 'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "\n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain precision/recall and F1 score '''\n",
    "    _conlleval = 'conlleval.pl'\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename).read())\n",
    "    for line in stdout.split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "\n",
    "    precision = float(out[6][:-2])\n",
    "    recall    = float(out[8][:-2])\n",
    "    f1score   = float(out[10])\n",
    "\n",
    "    return (precision, recall, f1score)\n",
    "\n",
    "class MyNNClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def inference(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, output_dimension):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_linear = nn.Linear(num_input_nodes, num_hidden_nodes)\n",
    "        self.output_linear = nn.Linear(num_hidden_nodes, output_dimension)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        out = self.input_linear(input_vector)\n",
    "        out = F.tanh(out)\n",
    "        out = self.output_linear(out)\n",
    "        out = F.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument(\"--data\", type=str, default=\"atis.small.pkl.gz\", help=\"The zipped dataset\")\n",
    "\n",
    "# parsed_args = argparser.parse_args(sys.argv[1:])\n",
    "\n",
    "filename = \"atis.small.pkl.gz\"\n",
    "f = gzip.open(filename,'rb')\n",
    "train_set, valid_set, test_set, dicts = cPickle.load(f)\n",
    "\n",
    "# print \"train_set \", train_set\n",
    "\n",
    "train_lex, _, train_y = train_set\n",
    "valid_lex, _, valid_y = valid_set\n",
    "test_lex,  _,  test_y  = test_set\n",
    "\n",
    "# print \"train_lex \", train_lex\n",
    "# print \"train_y \", train_y\n",
    "\n",
    "idx2label = dict((k,v) for v,k in dicts['labels2idx'].iteritems())\n",
    "idx2word  = dict((k,v) for v,k in dicts['words2idx'].iteritems())\n",
    "\n",
    "'''\n",
    "To have a look what the original data look like, commnet them before your submission\n",
    "'''\n",
    "print \"length train data \", len(train_lex), \" \", len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(train_x, train_y):\n",
    "    NUM_LABELS = len(idx2label)\n",
    "    VOCAB_SIZE = len(idx2word)\n",
    "    word_embedding_list = []\n",
    "    label_list = []\n",
    "    \n",
    "#     word_embeddings = torch.rand(VOCAB_SIZE, 300)\n",
    "    word_embeddings = torch.eye(VOCAB_SIZE, VOCAB_SIZE)\n",
    "    print \"VOCAB SIZE\", VOCAB_SIZE\n",
    "    print \"NUM LABELS \", NUM_LABELS\n",
    "    # tag_embeddings = torch.rand(NUM_LABELS+1, 100)\n",
    "    tag_embeddings = torch.eye(NUM_LABELS+1, NUM_LABELS+1)\n",
    "    for sentence, labels in zip(train_x, train_y):\n",
    "        prev_label = tag_embeddings[NUM_LABELS]\n",
    "        for word, label in zip(sentence, labels):\n",
    "            word_embedding = word_embeddings[word]\n",
    "            input_vector = torch.cat((word_embedding.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "            word_embedding_list.append(input_vector)\n",
    "            prev_label = tag_embeddings[label]\n",
    "            label_tensor = torch.LongTensor(NUM_LABELS).zero_().view(1,-1)\n",
    "            label_tensor[0,label] = 1\n",
    "            label_list.append(label_tensor)\n",
    "    print \"word embedding list \", len(word_embedding_list)\n",
    "    print \"label list \", len(label_list)\n",
    "#     print \"label list 0 \", label_list[0]\n",
    "    return word_embedding_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_list, label_list = create_embedding(train_lex, train_y)\n",
    "\n",
    "'''\n",
    "implement you training loop here\n",
    "'''\n",
    "# NUM_LABELS = len(idx2label)\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "HIDDEN_NODES = 200\n",
    "NUM_LABELS = len(idx2label)\n",
    "word_embedding_list = torch.stack(word_embedding_list)\n",
    "word_embedding_list = torch.squeeze(word_embedding_list)\n",
    "label_list = torch.stack(label_list)\n",
    "label_list = torch.squeeze(label_list)\n",
    "label_list = label_list.float()\n",
    "NUM_INPUT_NODES = word_embedding_list[0].size()[0]\n",
    "print \"number of input nodes \", NUM_INPUT_NODES\n",
    "print \"word_embeddings \", word_embedding_list.size()\n",
    "print \"label list \", label_list.size()\n",
    "\n",
    "model = NeuralNet(NUM_INPUT_NODES, HIDDEN_NODES, NUM_LABELS)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=10\n",
    "#                       , momentum=0.9\n",
    "                     )\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "words = autograd.Variable(word_embedding_list\n",
    "#                           , requires_grad=True\n",
    "                         )\n",
    "label = autograd.Variable(label_list\n",
    "                          , requires_grad=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    probs = model(words)\n",
    "    loss = loss_function(probs, label)\n",
    "    print \"loss \", loss.data[0]\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(word_embeddings, prev_label, word):\n",
    "    word_embedding = word_embeddings[word]\n",
    "    word_feature = autograd.Variable(word_embeddings[word])\n",
    "    prev_label = autograd.Variable(prev_label)\n",
    "    input_vector = torch.cat((word_feature.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "#     print \"input vector \", input_vector\n",
    "    return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "    output_labels = np.zeros(len(sentence))\n",
    "    prev_label = tag_embeddings[NUM_LABELS]\n",
    "    prob_list = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        input_vector =  get_input_vector(word_embeddings, prev_label, word)\n",
    "        probs = model(input_vector)\n",
    "#         print \"probs greedy\", probs\n",
    "        prob_list.append(probs)\n",
    "        max_val, predicted_label = torch.max(probs, 1)\n",
    "        predicted_label = predicted_label.data[0]\n",
    "        prev_label = tag_embeddings[predicted_label]\n",
    "        output_labels[i] = predicted_label\n",
    "#     print prob_list\n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = [ map(lambda t: idx2label[t], \n",
    "                             greedy_inference(model, x, \n",
    "                                              torch.eye(len(idx2word), len(idx2word))\n",
    "#                                               torch.rand(VOCAB_SIZE, 300)\n",
    "                                              ,\n",
    "                                        torch.eye(NUM_LABELS+1, NUM_LABELS+1), NUM_LABELS)) \n",
    "                        for x in test_lex\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"predictions \", predictions_test[0]\n",
    "groundtruth_test = [ map(lambda t: idx2label[t], y) for y in test_y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"groundtruth \", groundtruth_test[0]\n",
    "words_test = [ map(lambda t: idx2word[t], w) for w in test_lex ]\n",
    "test_precision, test_recall, test_f1score = conlleval(predictions_test, groundtruth_test, words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test_precision, test_recall, test_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
