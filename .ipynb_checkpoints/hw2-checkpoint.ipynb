{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement train\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Train method not implemented\")\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement inference\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Inference method not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "def conlleval(p, g, w, filename='tempfile.txt'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, ww in zip(sl, sp, sw):\n",
    "            out += ww + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename, 'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "\n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain precision/recall and F1 score '''\n",
    "    _conlleval = 'conlleval.pl'\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename).read())\n",
    "    for line in stdout.split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "\n",
    "    precision = float(out[6][:-2])\n",
    "    recall    = float(out[8][:-2])\n",
    "    f1score   = float(out[10])\n",
    "\n",
    "    return (precision, recall, f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, size):\n",
    "    # print \"label is \", label\n",
    "    tensor = torch.zeros(size)\n",
    "    tensor = tensor.long()\n",
    "    tensor[label] = 1\n",
    "    # print \"tensor is \", tensor\n",
    "    return tensor.view(1,-1)\n",
    "    # return torch.LongTensor([label.tolist()])\n",
    "\n",
    "\n",
    "class MyNNClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def inference(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, output_dimension):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_linear = nn.Linear(num_input_nodes, num_hidden_nodes)\n",
    "        self.middle_linear = nn.ReLU()\n",
    "        self.output_linear = nn.Linear(num_hidden_nodes, output_dimension)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        out = self.input_linear(input_vector)\n",
    "        h_relu = self.middle_linear(out)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        # return F.log_softmax(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train data  3983   3983\n",
      "epoch number  0  epoch_loss  22087.1813435\n"
     ]
    }
   ],
   "source": [
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument(\"--data\", type=str, default=\"atis.small.pkl.gz\", help=\"The zipped dataset\")\n",
    "\n",
    "# parsed_args = argparser.parse_args(sys.argv[1:])\n",
    "\n",
    "filename = \"atis.small.pkl.gz\"\n",
    "f = gzip.open(filename,'rb')\n",
    "train_set, valid_set, test_set, dicts = cPickle.load(f)\n",
    "\n",
    "# print \"train_set \", train_set\n",
    "\n",
    "train_lex, _, train_y = train_set\n",
    "valid_lex, _, valid_y = valid_set\n",
    "test_lex,  _,  test_y  = test_set\n",
    "\n",
    "# print \"train_lex \", train_lex\n",
    "# print \"train_y \", train_y\n",
    "\n",
    "idx2label = dict((k,v) for v,k in dicts['labels2idx'].iteritems())\n",
    "idx2word  = dict((k,v) for v,k in dicts['words2idx'].iteritems())\n",
    "\n",
    "'''\n",
    "To have a look what the original data look like, commnet them before your submission\n",
    "'''\n",
    "print \"length train data \", len(train_lex), \" \", len(train_y)\n",
    "# print \"test lex \", test_lex[0]\n",
    "# print \"word dictionary is \", idx2word\n",
    "# print \"label dictionary is \", idx2label\n",
    "# print train_lex[0], map(lambda t: idx2word[t], train_lex[0])\n",
    "# print train_y[0], map(lambda t: idx2label[t], train_y[0])\n",
    "# print test_lex[0], map(lambda t: idx2word[t], test_lex[0])\n",
    "# print test_y[0], map(lambda t: idx2label[t], test_y[0])\n",
    "\n",
    "'''\n",
    "implement you training loop here\n",
    "'''\n",
    "NUM_LABELS = len(idx2label)\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "HIDDEN_NODES = 150\n",
    "# word_embeddings = torch.rand(VOCAB_SIZE, 300)\n",
    "word_embeddings = torch.eye(VOCAB_SIZE, VOCAB_SIZE)\n",
    "# tag_embeddings = torch.rand(NUM_LABELS+1, 100)\n",
    "tag_embeddings = torch.eye(NUM_LABELS+1, NUM_LABELS+1)\n",
    "NUM_INPUT_NODES = len(word_embeddings[0]) + len(tag_embeddings[0])\n",
    "# print \"number of input nodes \", NUM_INPUT_NODES\n",
    "# print \"word_embeddings \", word_embeddings\n",
    "# print \"tag_embeddings \", tag_embeddings\n",
    "# input dimension for neural network is concatenation of word and tag tensors\n",
    "model = NeuralNet(NUM_INPUT_NODES, HIDDEN_NODES, NUM_LABELS)\n",
    "\n",
    "# loss_function = nn.MSELoss(size_average=False)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for sentence, labels in zip(train_lex, train_y):\n",
    "#         flag = bool(random.getrandbits(1))\n",
    "#         if flag:\n",
    "#             continue\n",
    "        prev_label = tag_embeddings[NUM_LABELS]\n",
    "        for word, label in zip(sentence, labels):\n",
    "            model.zero_grad()\n",
    "            word_feature = autograd.Variable(word_embeddings[word])\n",
    "            prev_label = autograd.Variable(prev_label)\n",
    "            input_vector = torch.cat((word_feature.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "\n",
    "            prev_label = tag_embeddings[label]\n",
    "            # input_vector = autograd.Variable(concat_vec)\n",
    "            # print \"input vector \", input_vector\n",
    "            label_tensor = torch.LongTensor([label.item()])\n",
    "            target = autograd.Variable(label_tensor)\n",
    "#             target = autograd.Variable(make_target(label, NUM_LABELS))\n",
    "            probs = model(input_vector)\n",
    "#             print \"probs \", log_probs\n",
    "#             print \"target \", target\n",
    "            loss = loss_function(probs, target)\n",
    "            epoch_loss += loss.data[0]\n",
    "#             print \"loss \", loss \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print \"epoch number \", epoch, \" epoch_loss \", epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_lex:\n",
    "        dp = np.zeros((NUM_LABELS, len(sentence)))\n",
    "        back_pointers = np.zeros((len(sentence), NUM_LABELS))\n",
    "        dp[0][0] = 1\n",
    "        for i in range(len(sentence)):\n",
    "            max_vector = []\n",
    "            back_point_vector = []\n",
    "            word_feature = autograd.Variable(word_embeddings[sentence[i]])\n",
    "            word_table = np.zeros((NUM_LABELS, NUM_LABELS))\n",
    "\n",
    "            prev_label =  tag_embeddings[NUM_LABELS]\n",
    "            for j in range(NUM_LABELS):\n",
    "                prev_label = autograd.Variable(prev_label)\n",
    "                input_vector = torch.cat((word_feature.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "#                 print \" probs \", probs\n",
    "#                 print \"dp array \", dp[:,i]\n",
    "#                 print np.multiply(dp[:, i], probs)\n",
    "                word_table[:,j] = np.multiply(dp[:, i],probs)\n",
    "                prev_label = tag_embeddings[j]\n",
    "            print \"word table is \", word_table\n",
    "            dp[:,i+1] = word_table.max(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
