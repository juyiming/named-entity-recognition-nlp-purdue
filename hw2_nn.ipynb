{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement train\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Train method not implemented\")\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement inference\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Inference method not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "def conlleval(p, g, w, filename='tempfile.txt'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, ww in zip(sl, sp, sw):\n",
    "            out += ww + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename, 'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "\n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain precision/recall and F1 score '''\n",
    "    _conlleval = 'conlleval.pl'\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename).read())\n",
    "    for line in stdout.split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "\n",
    "    precision = float(out[6][:-2])\n",
    "    recall    = float(out[8][:-2])\n",
    "    f1score   = float(out[10])\n",
    "\n",
    "    return (precision, recall, f1score)\n",
    "\n",
    "class MyNNClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def inference(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, output_dimension):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_linear = nn.Linear(num_input_nodes, num_hidden_nodes)\n",
    "        self.output_linear = nn.Linear(num_hidden_nodes, output_dimension)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        out = self.input_linear(input_vector)\n",
    "        out = F.tanh(out)\n",
    "        out = self.output_linear(out)\n",
    "        out = F.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train data  3983   3983\n"
     ]
    }
   ],
   "source": [
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument(\"--data\", type=str, default=\"atis.small.pkl.gz\", help=\"The zipped dataset\")\n",
    "\n",
    "# parsed_args = argparser.parse_args(sys.argv[1:])\n",
    "\n",
    "filename = \"atis.small.pkl.gz\"\n",
    "f = gzip.open(filename,'rb')\n",
    "train_set, valid_set, test_set, dicts = cPickle.load(f)\n",
    "\n",
    "# print \"train_set \", train_set\n",
    "\n",
    "train_lex, _, train_y = train_set\n",
    "valid_lex, _, valid_y = valid_set\n",
    "test_lex,  _,  test_y  = test_set\n",
    "\n",
    "# print \"train_lex \", train_lex\n",
    "# print \"train_y \", train_y\n",
    "\n",
    "idx2label = dict((k,v) for v,k in dicts['labels2idx'].iteritems())\n",
    "idx2word  = dict((k,v) for v,k in dicts['words2idx'].iteritems())\n",
    "\n",
    "'''\n",
    "To have a look what the original data look like, commnet them before your submission\n",
    "'''\n",
    "print \"length train data \", len(train_lex), \" \", len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = torch.rand(VOCAB_SIZE, 300)\n",
    "#     word_embeddings = torch.eye(VOCAB_SIZE, VOCAB_SIZE)\n",
    "tag_embeddings = torch.eye(NUM_LABELS+1, NUM_LABELS+1)\n",
    "def create_embedding(train_x, train_y):\n",
    "    NUM_LABELS = len(idx2label)\n",
    "    VOCAB_SIZE = len(idx2word)\n",
    "    word_embedding_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for sentence, labels in zip(train_x, train_y):\n",
    "        prev_label = tag_embeddings[NUM_LABELS]\n",
    "        for word, label in zip(sentence, labels):\n",
    "            word_embedding = word_embeddings[word]\n",
    "            input_vector = torch.cat((word_embedding.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "            word_embedding_list.append(input_vector)\n",
    "            prev_label = tag_embeddings[label]\n",
    "            # for mse loss\n",
    "            label_tensor = torch.LongTensor(NUM_LABELS).zero_().view(1,-1)\n",
    "            label_tensor[0,label] = 1\n",
    "            label_tensor = label_tensor.float()\n",
    "            label_list.append(label_tensor)\n",
    "            \n",
    "            # for cross entropy loss since multi target not supported\n",
    "#             label_tensor = torch.LongTensor([label.item()])\n",
    "#             label_tensor = label_tensor.long()\n",
    "#             label_list.append(label_tensor)\n",
    "    print \"word embedding list \", len(word_embedding_list)\n",
    "    print \"label list \", len(label_list)\n",
    "#     print \"label list 0 \", label_list[0]\n",
    "    return word_embedding_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embedding list  45388\n",
      "label list  45388\n",
      "number of input nodes  428\n",
      "word_embeddings  torch.Size([45388, 428])\n",
      "label list  torch.Size([45388, 127])\n"
     ]
    }
   ],
   "source": [
    "word_embedding_list, label_list = create_embedding(train_lex, train_y)\n",
    "\n",
    "'''\n",
    "implement you training loop here\n",
    "'''\n",
    "# NUM_LABELS = len(idx2label)\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "HIDDEN_NODES = 1000\n",
    "NUM_LABELS = len(idx2label)\n",
    "word_embedding_list = torch.stack(word_embedding_list)\n",
    "word_embedding_list = torch.squeeze(word_embedding_list)\n",
    "label_list = torch.stack(label_list)\n",
    "label_list = torch.squeeze(label_list)\n",
    "NUM_INPUT_NODES = word_embedding_list[0].size()[0]\n",
    "print \"number of input nodes \", NUM_INPUT_NODES\n",
    "print \"word_embeddings \", word_embedding_list.size()\n",
    "print \"label list \", label_list.size()\n",
    "\n",
    "model = NeuralNet(NUM_INPUT_NODES, HIDDEN_NODES, NUM_LABELS)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1\n",
    "# #                       , momentum=0.9\n",
    "#                      )\n",
    "words = autograd.Variable(word_embedding_list\n",
    "#                           , requires_grad=True\n",
    "                         )\n",
    "label = autograd.Variable(label_list\n",
    "                          , requires_grad=False\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  0.00133857852779\n",
      "loss  0.00133808888495\n",
      "loss  0.00133757316507\n",
      "loss  0.00133706117049\n",
      "loss  0.001336540794\n",
      "loss  0.00133599853143\n",
      "loss  0.00133547571022\n",
      "loss  0.00133495568298\n",
      "loss  0.00133446638938\n",
      "loss  0.00133399281185\n",
      "loss  0.00133343169\n",
      "loss  0.00133290269878\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-449-95e193ffd1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"loss \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2ff002f6f17e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_vector)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_autograd_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/pointwise.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, inplace)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    probs = model(words)\n",
    "    loss = loss_function(probs, label)\n",
    "    print \"loss \", loss.data[0]\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(word_embeddings, prev_label, word):\n",
    "    word_embedding = word_embeddings[word]\n",
    "    word_feature = autograd.Variable(word_embeddings[word])\n",
    "    prev_label = autograd.Variable(prev_label)\n",
    "    input_vector = torch.cat((word_feature.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "#     print \"input vector \", input_vector\n",
    "    return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "    output_labels = np.zeros(len(sentence))\n",
    "    prev_label = tag_embeddings[NUM_LABELS]\n",
    "    prob_list = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        input_vector =  get_input_vector(word_embeddings, prev_label, word)\n",
    "        probs = model(input_vector)\n",
    "#         print \"probs greedy\", probs.data\n",
    "        prob_list.append(probs)\n",
    "        max_val, predicted_label = torch.max(probs, 1)\n",
    "        predicted_label = predicted_label.data[0]\n",
    "        prev_label = tag_embeddings[predicted_label]\n",
    "        output_labels[i] = predicted_label\n",
    "    print output_labels\n",
    "    print prob_list\n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "        dp = np.zeros((NUM_LABELS, len(sentence)+1))\n",
    "        back_pointers = np.zeros((NUM_LABELS, len(sentence)))\n",
    "        dp[0][0] = 1\n",
    "        for i in range(len(sentence)):\n",
    "            word_table = np.zeros((NUM_LABELS, NUM_LABELS))\n",
    "            if i == 0:\n",
    "                input_vector = get_input_vector(word_embeddings, tag_embeddings[NUM_LABELS], sentence[i])\n",
    "#                 print \"input_vector \", input_vector\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "#                 print \"probs is \", probs\n",
    "#                 word_table[:,0] = np.log(dp[0, 0]) + np.log(probs)\n",
    "#                 word_table[:,0] = dp[0,0] + probs\n",
    "                word_table[:,0] = np.multiply(dp[0, 0], probs)\n",
    "#                 word_table[:,0] = np.log(dp[0, 0]) + np.log(probs)\n",
    "#                 print \"word table 0\", word_table[:, 0]\n",
    "                dp[:,i+1] = word_table[:,0]\n",
    "                back_pointers[:,i] = 128\n",
    "                continue\n",
    "#             print i\n",
    "            for j in range(NUM_LABELS):\n",
    "                input_vector = get_input_vector(word_embeddings, tag_embeddings[j], sentence[i])\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "#                 if j == 126:\n",
    "#                     print \" probs \", probs\n",
    "#                 print \"dp array \", dp[:,i]\n",
    "#                     print \"np product \", np.multiply(dp[j, i], probs)\n",
    "#                 word_table[:,j] = dp[:, i] + probs\n",
    "                word_table[:,j] = np.multiply(dp[j, i], probs)\n",
    "#                 word_table[:,j] = np.log(dp[:, i]) + np.log(probs)\n",
    "#             for ind, array in enumerate(word_table):\n",
    "#                 print \"word table index \", ind, \" \", array\n",
    "            dp[:,i+1] = word_table.max(1)\n",
    "#             print \"dp \", i+1, \" \", dp[:,i+1]\n",
    "#             break\n",
    "            for k in range(NUM_LABELS):\n",
    "                for index, element in enumerate(word_table[k]):\n",
    "                    if element == dp[k, i+1]:\n",
    "                        back_pointers[k, i] = index\n",
    "#                 back_pointers[k, i] = word_table[k].index(dp[k,i+1])\n",
    "\n",
    "#         print \"back_pointers \", back_pointers\n",
    "#         print \"dp matrix \", dp[:,19]\n",
    "        output_labels = np.zeros(len(sentence), dtype = np.int)\n",
    "        label_index = len(sentence) - 1\n",
    "        max_val = dp[:, len(sentence)].max()\n",
    "        for index, element in enumerate(dp[:, len(sentence)]):\n",
    "            if element == max_val:\n",
    "                output_labels[label_index] = index\n",
    "#                 print \"debug2 \", index\n",
    "                break\n",
    "#         print \"output labels \", output_labels\n",
    "        for i in range(len(sentence)-1, 0, -1): #18 to 1\n",
    "#             print \"debug \", output_labels[label_index]\n",
    "            row = back_pointers[output_labels[label_index], i]\n",
    "            label_index -= 1\n",
    "            output_labels[label_index] = row\n",
    "#         for i in np.nditer(dp,order='F'):\n",
    "#             print i\n",
    "#         print [dp[:, i] for i in range(dp.shape[1])]\n",
    "#         print [back_pointers[:, i] for i in range(back_pointers.shape[1])]\n",
    "#         print \"output labels \", output_labels\n",
    "        return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_test = [ map(lambda t: idx2label[t], \n",
    "                             viterbi_inference(model, x, \n",
    "#                                               torch.eye(len(idx2word), len(idx2word))\n",
    "                                              word_embeddings\n",
    "                                              ,\n",
    "                                        tag_embeddings, NUM_LABELS)) \n",
    "                        for x in test_lex\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"predictions \", predictions_test[0]\n",
    "groundtruth_test = [ map(lambda t: idx2label[t], y) for y in test_y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"groundtruth \", groundtruth_test[0]\n",
    "words_test = [ map(lambda t: idx2word[t], w) for w in test_lex ]\n",
    "test_precision, test_recall, test_f1score = conlleval(predictions_test, groundtruth_test, words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test_precision, test_recall, test_f1score\n",
    "# print idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
