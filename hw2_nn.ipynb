{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement train\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Train method not implemented\")\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Override this method in your class to implement inference\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Inference method not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "def conlleval(p, g, w, filename='tempfile.txt'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, ww in zip(sl, sp, sw):\n",
    "            out += ww + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename, 'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "\n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain precision/recall and F1 score '''\n",
    "    _conlleval = 'conlleval.pl'\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename).read())\n",
    "    for line in stdout.split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "\n",
    "    precision = float(out[6][:-2])\n",
    "    recall    = float(out[8][:-2])\n",
    "    f1score   = float(out[10])\n",
    "\n",
    "    return (precision, recall, f1score)\n",
    "\n",
    "class MyNNClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def inference(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, output_dimension):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_linear = nn.Linear(num_input_nodes, num_hidden_nodes)\n",
    "        self.output_linear = nn.Linear(num_hidden_nodes, output_dimension)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        out = self.input_linear(input_vector)\n",
    "        out = F.tanh(out)\n",
    "        out = self.output_linear(out)\n",
    "        out = F.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train data  3983   3983\n"
     ]
    }
   ],
   "source": [
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument(\"--data\", type=str, default=\"atis.small.pkl.gz\", help=\"The zipped dataset\")\n",
    "\n",
    "# parsed_args = argparser.parse_args(sys.argv[1:])\n",
    "\n",
    "filename = \"atis.small.pkl.gz\"\n",
    "f = gzip.open(filename,'rb')\n",
    "train_set, valid_set, test_set, dicts = cPickle.load(f)\n",
    "\n",
    "# print \"train_set \", train_set\n",
    "\n",
    "train_lex, _, train_y = train_set\n",
    "valid_lex, _, valid_y = valid_set\n",
    "test_lex,  _,  test_y  = test_set\n",
    "\n",
    "# print \"train_lex \", train_lex\n",
    "# print \"train_y \", train_y\n",
    "\n",
    "idx2label = dict((k,v) for v,k in dicts['labels2idx'].iteritems())\n",
    "idx2word  = dict((k,v) for v,k in dicts['words2idx'].iteritems())\n",
    "\n",
    "'''\n",
    "To have a look what the original data look like, commnet them before your submission\n",
    "'''\n",
    "print \"length train data \", len(train_lex), \" \", len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(idx2word)\n",
    "word_embeddings = torch.rand(VOCAB_SIZE, 300)\n",
    "NUM_LABELS = len(idx2label)\n",
    "#     word_embeddings = torch.eye(VOCAB_SIZE, VOCAB_SIZE)\n",
    "tag_embeddings = torch.eye(NUM_LABELS+1, NUM_LABELS+1)\n",
    "def create_embedding(train_x, train_y):\n",
    "    NUM_LABELS = len(idx2label)\n",
    "    VOCAB_SIZE = len(idx2word)\n",
    "    word_embedding_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for sentence, labels in zip(train_x, train_y):\n",
    "        prev_label = tag_embeddings[NUM_LABELS]\n",
    "        for word, label in zip(sentence, labels):\n",
    "            word_embedding = word_embeddings[word]\n",
    "            input_vector = torch.cat((word_embedding.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "            word_embedding_list.append(input_vector)\n",
    "            prev_label = tag_embeddings[label]\n",
    "            # for mse loss\n",
    "            label_tensor = torch.LongTensor(NUM_LABELS).zero_().view(1,-1)\n",
    "            label_tensor[0,label] = 1\n",
    "            label_tensor = label_tensor.float()\n",
    "            label_list.append(label_tensor)\n",
    "            \n",
    "            # for cross entropy loss since multi target not supported\n",
    "#             label_tensor = torch.LongTensor([label.item()])\n",
    "#             label_tensor = label_tensor.long()\n",
    "#             label_list.append(label_tensor)\n",
    "    print \"word embedding list \", len(word_embedding_list)\n",
    "    print \"label list \", len(label_list)\n",
    "#     print \"label list 0 \", label_list[0]\n",
    "    return word_embedding_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embedding list  45388\n",
      "label list  45388\n",
      "number of input nodes  428\n",
      "word_embeddings  torch.Size([45388, 428])\n",
      "label list  torch.Size([45388, 127])\n"
     ]
    }
   ],
   "source": [
    "word_embedding_list, label_list = create_embedding(train_lex, train_y)\n",
    "\n",
    "'''\n",
    "implement you training loop here\n",
    "'''\n",
    "# NUM_LABELS = len(idx2label)\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "HIDDEN_NODES = 1000\n",
    "NUM_LABELS = len(idx2label)\n",
    "word_embedding_list = torch.stack(word_embedding_list)\n",
    "word_embedding_list = torch.squeeze(word_embedding_list)\n",
    "label_list = torch.stack(label_list)\n",
    "label_list = torch.squeeze(label_list)\n",
    "NUM_INPUT_NODES = word_embedding_list[0].size()[0]\n",
    "print \"number of input nodes \", NUM_INPUT_NODES\n",
    "print \"word_embeddings \", word_embedding_list.size()\n",
    "print \"label list \", label_list.size()\n",
    "\n",
    "model = NeuralNet(NUM_INPUT_NODES, HIDDEN_NODES, NUM_LABELS)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1\n",
    "# #                       , momentum=0.9\n",
    "#                      )\n",
    "words = autograd.Variable(word_embedding_list\n",
    "#                           , requires_grad=True\n",
    "                         )\n",
    "label = autograd.Variable(label_list\n",
    "                          , requires_grad=False\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  0.000886002439074\n",
      "loss  0.000885973044205\n",
      "loss  0.000885978806764\n",
      "loss  0.000885977642611\n",
      "loss  0.000885967048816\n",
      "loss  0.000885956105776\n",
      "loss  0.000885950110387\n",
      "loss  0.000885917805135\n",
      "loss  0.000885944697075\n",
      "loss  0.000885924324393\n",
      "loss  0.00088591966778\n",
      "loss  0.000885897141416\n",
      "loss  0.000885898247361\n",
      "loss  0.00088587513892\n",
      "loss  0.000885880144779\n",
      "loss  0.000885872053914\n",
      "loss  0.000885855697561\n",
      "loss  0.000885844347067\n",
      "loss  0.000885824905708\n",
      "loss  0.000885834742803\n",
      "loss  0.000885797722731\n",
      "loss  0.000885804416612\n",
      "loss  0.000885781948455\n",
      "loss  0.000885791494511\n",
      "loss  0.000885787769221\n",
      "loss  0.000885769084562\n",
      "loss  0.000885749934241\n",
      "loss  0.000885756628122\n",
      "loss  0.000885751622263\n",
      "loss  0.000885743647814\n",
      "loss  0.000885732413735\n",
      "loss  0.000885736430064\n",
      "loss  0.000885726360139\n",
      "loss  0.000885704706889\n",
      "loss  0.000885700283106\n",
      "loss  0.000885676825419\n",
      "loss  0.000885666930117\n",
      "loss  0.00088565639453\n",
      "loss  0.000885648536496\n",
      "loss  0.000885648129042\n",
      "loss  0.000885628105607\n",
      "loss  0.000885619898327\n",
      "loss  0.000885608897079\n",
      "loss  0.000885604822543\n",
      "loss  0.000885588873643\n",
      "loss  0.000885591434781\n",
      "loss  0.000885579909664\n",
      "loss  0.000885566871148\n",
      "loss  0.000885543995537\n",
      "loss  0.000885532295797\n",
      "loss  0.000885514833499\n",
      "loss  0.000885519664735\n",
      "loss  0.000885494169779\n",
      "loss  0.000885496789124\n",
      "loss  0.000885466753971\n",
      "loss  0.000885465880856\n",
      "loss  0.000885455985554\n",
      "loss  0.000885457440745\n",
      "loss  0.000885458954144\n",
      "loss  0.000885421002749\n",
      "loss  0.00088539958233\n",
      "loss  0.000885431596544\n",
      "loss  0.000885411922354\n",
      "loss  0.00088539608987\n",
      "loss  0.000885395915247\n",
      "loss  0.000885384914\n",
      "loss  0.000885414367076\n",
      "loss  0.000885395566002\n",
      "loss  0.00088539061835\n",
      "loss  0.000885375600774\n",
      "loss  0.000885352317709\n",
      "loss  0.000885330431629\n",
      "loss  0.000885314249899\n",
      "loss  0.000885297427885\n",
      "loss  0.000885287590791\n",
      "loss  0.000885317334905\n",
      "loss  0.00088527490152\n",
      "loss  0.000885266577825\n",
      "loss  0.000885271001607\n",
      "loss  0.000885248125996\n",
      "loss  0.000885247020051\n",
      "loss  0.000885228218976\n",
      "loss  0.00088522938313\n",
      "loss  0.000885175948497\n",
      "loss  0.000885200337507\n",
      "loss  0.000885180430487\n",
      "loss  0.000885168090463\n",
      "loss  0.000885166868102\n",
      "loss  0.000885147834197\n",
      "loss  0.000885143817868\n",
      "loss  0.000885126064532\n",
      "loss  0.000885105051566\n",
      "loss  0.000885125540663\n",
      "loss  0.000885114539415\n",
      "loss  0.000885115819983\n",
      "loss  0.00088509375928\n",
      "loss  0.000885062618181\n",
      "loss  0.000885051325895\n",
      "loss  0.000885051500518\n",
      "loss  0.000885063549504\n",
      "loss  0.00088505842723\n",
      "loss  0.000885037705302\n",
      "loss  0.000884998240508\n",
      "loss  0.000885035900865\n",
      "loss  0.000884993991349\n",
      "loss  0.00088497280376\n",
      "loss  0.000884974258952\n",
      "loss  0.00088496907847\n",
      "loss  0.000884953595232\n",
      "loss  0.000884951732587\n",
      "loss  0.000884948822204\n",
      "loss  0.000884909473825\n",
      "loss  0.000884919194505\n",
      "loss  0.000884905224666\n",
      "loss  0.000884885375854\n",
      "loss  0.000884887587745\n",
      "loss  0.000884899985977\n",
      "loss  0.00088490935741\n",
      "loss  0.000884872162715\n",
      "loss  0.00088485411834\n",
      "loss  0.000884810579009\n",
      "loss  0.000884820998181\n",
      "loss  0.000884823792148\n",
      "loss  0.000884820939973\n",
      "loss  0.000884833279997\n",
      "loss  0.000884794688318\n",
      "loss  0.000884791370481\n",
      "loss  0.000884782115463\n",
      "loss  0.00088474387303\n",
      "loss  0.000884772627614\n",
      "loss  0.000884765351657\n",
      "loss  0.000884741893969\n",
      "loss  0.000884769775439\n",
      "loss  0.00088476104429\n",
      "loss  0.000884721172042\n",
      "loss  0.00088472123025\n",
      "loss  0.000884715409484\n",
      "loss  0.00088469410548\n",
      "loss  0.00088470504852\n",
      "loss  0.000884681241587\n",
      "loss  0.000884670997038\n",
      "loss  0.000884694687556\n",
      "loss  0.000884678389411\n",
      "loss  0.000884660170414\n",
      "loss  0.000884645909537\n",
      "loss  0.000884622801095\n",
      "loss  0.000884639332071\n",
      "loss  0.000884603243321\n",
      "loss  0.000884611625224\n",
      "loss  0.000884590379428\n",
      "loss  0.000884597364347\n",
      "loss  0.000884588574991\n",
      "loss  0.000884586712345\n",
      "loss  0.000884581007995\n",
      "loss  0.000884553650394\n",
      "loss  0.000884558015969\n",
      "loss  0.000884546840098\n",
      "loss  0.00088454055367\n",
      "loss  0.000884523440618\n",
      "loss  0.000884528853931\n",
      "loss  0.000884510751348\n",
      "loss  0.000884511508048\n",
      "loss  0.000884494802449\n",
      "loss  0.000884501321707\n",
      "loss  0.000884475768544\n",
      "loss  0.00088446913287\n",
      "loss  0.000884462089743\n",
      "loss  0.00088445196161\n",
      "loss  0.000884443812538\n",
      "loss  0.000884429260623\n",
      "loss  0.000884424138349\n",
      "loss  0.00088442111155\n",
      "loss  0.000884410343133\n",
      "loss  0.000884400506038\n",
      "loss  0.000884404580574\n",
      "loss  0.000884380715434\n",
      "loss  0.000884378852788\n",
      "loss  0.000884364708327\n",
      "loss  0.000884363194928\n",
      "loss  0.000884338282049\n",
      "loss  0.000884343404323\n",
      "loss  0.000884322158527\n",
      "loss  0.0008843143587\n",
      "loss  0.000884290260728\n",
      "loss  0.000884294218849\n",
      "loss  0.000884277455043\n",
      "loss  0.00088425836293\n",
      "loss  0.000884257548023\n",
      "loss  0.000884254288394\n",
      "loss  0.000884243636392\n",
      "loss  0.000884229841176\n",
      "loss  0.000884228968062\n",
      "loss  0.000884201726876\n",
      "loss  0.000884191773366\n",
      "loss  0.00088416953804\n",
      "loss  0.000884166918695\n",
      "loss  0.000884159118868\n",
      "loss  0.00088415335631\n",
      "loss  0.000884146662429\n",
      "loss  0.000884143519215\n",
      "loss  0.000884125125594\n",
      "loss  0.000884124950971\n",
      "loss  0.000884095905349\n",
      "loss  0.000884108187165\n",
      "loss  0.000884095672518\n",
      "loss  0.000884123786818\n",
      "loss  0.00088411080651\n",
      "loss  0.000884108943865\n",
      "loss  0.000884091015905\n",
      "loss  0.000884041946847\n",
      "loss  0.000884033623151\n",
      "loss  0.000883987580892\n",
      "loss  0.000883992062882\n",
      "loss  0.000883994274773\n",
      "loss  0.000883972388692\n",
      "loss  0.000883986824192\n",
      "loss  0.000883950327989\n",
      "loss  0.000883932283614\n",
      "loss  0.000883911445271\n",
      "loss  0.000883905799128\n",
      "loss  0.000883918371983\n",
      "loss  0.000883899745531\n",
      "loss  0.000883876520675\n",
      "loss  0.000883874075953\n",
      "loss  0.000883845845237\n",
      "loss  0.000883873202838\n",
      "loss  0.000883849919774\n",
      "loss  0.000883843225893\n",
      "loss  0.000883831642568\n",
      "loss  0.000883825705387\n",
      "loss  0.000883794098627\n",
      "loss  0.00088380521629\n",
      "loss  0.000883807253558\n",
      "loss  0.00088376714848\n",
      "loss  0.000883791130036\n",
      "loss  0.000883772911038\n",
      "loss  0.000883777567651\n",
      "loss  0.000883776985575\n",
      "loss  0.000883737753611\n",
      "loss  0.000883769185748\n",
      "loss  0.000883720058482\n",
      "loss  0.000883726985194\n",
      "loss  0.000883756496478\n",
      "loss  0.000883727800101\n",
      "loss  0.000883709231857\n",
      "loss  0.000883709231857\n",
      "loss  0.00088370317826\n",
      "loss  0.000883688568138\n",
      "loss  0.000883684901055\n",
      "loss  0.000883703120053\n",
      "loss  0.000883675180376\n",
      "loss  0.000883669767063\n",
      "loss  0.000883677043021\n",
      "loss  0.000883669243194\n",
      "loss  0.00088366511045\n",
      "loss  0.000883636297658\n",
      "loss  0.000883650965989\n",
      "loss  0.000883635773789\n",
      "loss  0.000883641769178\n",
      "loss  0.000883625471033\n",
      "loss  0.000883636705112\n",
      "loss  0.000883624656126\n",
      "loss  0.0008836153429\n",
      "loss  0.000883617729414\n",
      "loss  0.000883617962245\n",
      "loss  0.000883608008735\n",
      "loss  0.0008836016641\n",
      "loss  0.000883592409082\n",
      "loss  0.000883596367203\n",
      "loss  0.000883583445102\n",
      "loss  0.000883580534719\n",
      "loss  0.000883576925844\n",
      "loss  0.00088355934713\n",
      "loss  0.000883558881469\n",
      "loss  0.000883543805685\n",
      "loss  0.000883538683411\n",
      "loss  0.000883555156179\n",
      "loss  0.000883530417923\n",
      "loss  0.000883526867256\n",
      "loss  0.000883528380655\n",
      "loss  0.000883513886947\n",
      "loss  0.000883520580828\n",
      "loss  0.000883502536453\n",
      "loss  0.000883509288542\n",
      "loss  0.000883484259248\n",
      "loss  0.000883494503796\n",
      "loss  0.000883487053216\n",
      "loss  0.000883480650373\n",
      "loss  0.000883474480361\n",
      "loss  0.000883488275576\n",
      "loss  0.000883469474502\n",
      "loss  0.000883466622327\n",
      "loss  0.000883457658347\n",
      "loss  0.000883453816641\n",
      "loss  0.000883439264726\n",
      "loss  0.000883460161276\n",
      "loss  0.000883446715306\n",
      "loss  0.000883431523107\n",
      "loss  0.000883419183083\n",
      "loss  0.000883417320438\n",
      "loss  0.000883410451934\n",
      "loss  0.000883398752194\n",
      "loss  0.000883393571712\n",
      "loss  0.000883384956978\n",
      "loss  0.000883401662577\n",
      "loss  0.000883392815012\n",
      "loss  0.000883375236299\n",
      "loss  0.00088336825138\n",
      "loss  0.000883376342244\n",
      "loss  0.00088336202316\n",
      "loss  0.000883366214111\n",
      "loss  0.000883367843926\n",
      "loss  0.000883345725015\n",
      "loss  0.000883358356077\n",
      "loss  0.00088335108012\n",
      "loss  0.000883338449057\n",
      "loss  0.000883333559614\n",
      "loss  0.00088331871666\n",
      "loss  0.000883318658452\n",
      "loss  0.000883299508132\n",
      "loss  0.000883303233422\n",
      "loss  0.000883298460394\n",
      "loss  0.000883286527824\n",
      "loss  0.00088328687707\n",
      "loss  0.000883281463757\n",
      "loss  0.000883280532435\n",
      "loss  0.000883274828084\n",
      "loss  0.000883277622052\n",
      "loss  0.000883252127096\n",
      "loss  0.000883252243511\n",
      "loss  0.000883237808011\n",
      "loss  0.000883229018655\n",
      "loss  0.000883222208358\n",
      "loss  0.00088320649229\n",
      "loss  0.000883207307197\n",
      "loss  0.000883197470102\n",
      "loss  0.000883195956703\n",
      "loss  0.000883186643478\n",
      "loss  0.000883188680746\n",
      "loss  0.000883173197508\n",
      "loss  0.000883176515345\n",
      "loss  0.000883150903974\n",
      "loss  0.000883162021637\n",
      "loss  0.000883132277522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  0.000883124535903\n",
      "loss  0.000883118365891\n",
      "loss  0.000883103290107\n",
      "loss  0.000883101427462\n",
      "loss  0.000883076281752\n",
      "loss  0.000883093976881\n",
      "loss  0.000883067783434\n",
      "loss  0.000883064582013\n",
      "loss  0.00088304001838\n",
      "loss  0.000883040542249\n",
      "loss  0.000883042928763\n",
      "loss  0.000883048167452\n",
      "loss  0.000883033324499\n",
      "loss  0.000883034605067\n",
      "loss  0.000882992520928\n",
      "loss  0.00088298326591\n",
      "loss  0.000882992171682\n",
      "loss  0.000882970460225\n",
      "loss  0.000883005734067\n",
      "loss  0.000882959109731\n",
      "loss  0.00088296161266\n",
      "loss  0.000882946769707\n",
      "loss  0.000882980413735\n",
      "loss  0.00088299147319\n",
      "loss  0.000882958702277\n",
      "loss  0.000882955850102\n",
      "loss  0.000882914231624\n",
      "loss  0.000882904743776\n",
      "loss  0.00088287179824\n",
      "loss  0.000882855791133\n",
      "loss  0.000882875930984\n",
      "loss  0.000882869178895\n",
      "loss  0.00088284316007\n",
      "loss  0.00088284839876\n",
      "loss  0.000882796070073\n",
      "loss  0.000882796302903\n",
      "loss  0.000882773019839\n",
      "loss  0.000882781401742\n",
      "loss  0.000882773951162\n",
      "loss  0.000882738037035\n",
      "loss  0.000882729713339\n",
      "loss  0.000882756954525\n",
      "loss  0.000882713298779\n",
      "loss  0.000882723892573\n",
      "loss  0.000882707885467\n",
      "loss  0.000882696709596\n",
      "loss  0.000882702646777\n",
      "loss  0.000882691005245\n",
      "loss  0.000882664520759\n",
      "loss  0.000882649503183\n",
      "loss  0.000882653170265\n",
      "loss  0.000882656662725\n",
      "loss  0.000882633030415\n",
      "loss  0.000882621912751\n",
      "loss  0.000882621563505\n",
      "loss  0.000882625114173\n",
      "loss  0.000882600899786\n",
      "loss  0.000882568012457\n",
      "loss  0.000882568303496\n",
      "loss  0.000882596126758\n",
      "loss  0.000882560154423\n",
      "loss  0.000882556079887\n",
      "loss  0.000882557767909\n",
      "loss  0.000882524938788\n",
      "loss  0.000882521620952\n",
      "loss  0.000882529246155\n",
      "loss  0.000882499851286\n",
      "loss  0.00088250922272\n",
      "loss  0.00088248483371\n",
      "loss  0.000882477906998\n",
      "loss  0.000882478838321\n",
      "loss  0.00088245869847\n",
      "loss  0.000882446067408\n",
      "loss  0.000882423773874\n",
      "loss  0.00088243331993\n",
      "loss  0.000882413703948\n",
      "loss  0.000882404856384\n",
      "loss  0.000882413296495\n",
      "loss  0.000882384192664\n",
      "loss  0.000882386346348\n",
      "loss  0.000882366439328\n",
      "loss  0.000882363354322\n",
      "loss  0.000882349559106\n",
      "loss  0.000882337859366\n",
      "loss  0.000882349384483\n",
      "loss  0.000882314518094\n",
      "loss  0.000882322026882\n",
      "loss  0.000882299675141\n",
      "loss  0.00088230840629\n",
      "loss  0.000882302469108\n",
      "loss  0.000882282969542\n",
      "loss  0.000882274529431\n",
      "loss  0.000882252585143\n",
      "loss  0.00088225241052\n",
      "loss  0.000882235530298\n",
      "loss  0.000882230058778\n",
      "loss  0.000882226158865\n",
      "loss  0.000882235472091\n",
      "loss  0.000882225111127\n",
      "loss  0.000882210035343\n",
      "loss  0.000882190302946\n",
      "loss  0.000882188265678\n",
      "loss  0.000882175227161\n",
      "loss  0.000882177962922\n",
      "loss  0.000882171909325\n",
      "loss  0.000882172724232\n",
      "loss  0.000882166263182\n",
      "loss  0.00088215013966\n",
      "loss  0.000882158812601\n",
      "loss  0.000882141932379\n",
      "loss  0.000882133725099\n",
      "loss  0.000882127904333\n",
      "loss  0.000882120453753\n",
      "loss  0.000882117019501\n",
      "loss  0.000882108404767\n",
      "loss  0.000882115622517\n",
      "loss  0.000882113934495\n",
      "loss  0.000882096181158\n",
      "loss  0.000882074527908\n",
      "loss  0.000882087333594\n",
      "loss  0.000882077496499\n",
      "loss  0.000882066553459\n",
      "loss  0.000882054504473\n",
      "loss  0.000882055377588\n",
      "loss  0.000882041582372\n",
      "loss  0.000882046821062\n",
      "loss  0.000882032501977\n",
      "loss  0.000882034713868\n",
      "loss  0.000882023596205\n",
      "loss  0.000882025982719\n",
      "loss  0.00088203366613\n",
      "loss  0.00088200828759\n",
      "loss  0.000882002583239\n",
      "loss  0.00088199286256\n",
      "loss  0.000881984364241\n",
      "loss  0.000881995423697\n",
      "loss  0.000881980347913\n",
      "loss  0.000882004154846\n",
      "loss  0.000881978718098\n",
      "loss  0.000881975574885\n",
      "loss  0.000881969986949\n",
      "loss  0.000881967891473\n",
      "loss  0.000881964457221\n",
      "loss  0.000881959393155\n",
      "loss  0.00088195328135\n",
      "loss  0.000881953747012\n",
      "loss  0.000881943968125\n",
      "loss  0.000881944724824\n",
      "loss  0.000881944608409\n",
      "loss  0.000881933548953\n",
      "loss  0.0008819323848\n",
      "loss  0.000881920277607\n",
      "loss  0.000881917891093\n",
      "loss  0.000881900603417\n",
      "loss  0.000881910091266\n",
      "loss  0.000881892046891\n",
      "loss  0.000881887972355\n",
      "loss  0.000881875108462\n",
      "loss  0.000881878950167\n",
      "loss  0.000881864107214\n",
      "loss  0.000881848158315\n",
      "loss  0.000881850661244\n",
      "loss  0.000881866028067\n",
      "loss  0.000881862069946\n",
      "loss  0.000881851417944\n",
      "loss  0.000881857762579\n",
      "loss  0.000881852058228\n",
      "loss  0.000881833955646\n",
      "loss  0.000881835061591\n",
      "loss  0.00088185002096\n",
      "loss  0.000881816726178\n",
      "loss  0.000881814048626\n",
      "loss  0.000881834013853\n",
      "loss  0.000881831569131\n",
      "loss  0.000881815794855\n",
      "loss  0.000881850952283\n",
      "loss  0.000881839485373\n",
      "loss  0.000881847401615\n",
      "loss  0.00088184082415\n",
      "loss  0.000881835410837\n",
      "loss  0.000881801010109\n",
      "loss  0.000881769345142\n",
      "loss  0.000881767016836\n",
      "loss  0.000881793210283\n",
      "loss  0.000881763698999\n",
      "loss  0.000881770334672\n",
      "loss  0.000881770101842\n",
      "loss  0.000881762302015\n",
      "loss  0.000881744839717\n",
      "loss  0.000881764630321\n",
      "loss  0.000881754676811\n",
      "loss  0.000881730928086\n",
      "loss  0.000881751417182\n",
      "loss  0.00088169623632\n",
      "loss  0.000881697633304\n",
      "loss  0.00088169373339\n",
      "loss  0.000881698855665\n",
      "loss  0.000881704501808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-085fb14c5d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     count += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"loss \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2ff002f6f17e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_vector)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/linear.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "#     count += 1\n",
    "    probs = model(words)\n",
    "    loss = loss_function(probs, label)\n",
    "    print \"loss \", loss.data[0] \n",
    "#     + \"epoch \", count\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'parameters.pt')\n",
    "torch.save(word_embeddings, 'word_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(word_embeddings, prev_label, word):\n",
    "    word_embedding = word_embeddings[word]\n",
    "    word_feature = autograd.Variable(word_embeddings[word])\n",
    "    prev_label = autograd.Variable(prev_label)\n",
    "    input_vector = torch.cat((word_feature.view(1,-1), prev_label.view(1,-1)), 1)\n",
    "#     print \"input vector \", input_vector\n",
    "    return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "    output_labels = np.zeros(len(sentence))\n",
    "    prev_label = tag_embeddings[NUM_LABELS]\n",
    "    prob_list = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        input_vector =  get_input_vector(word_embeddings, prev_label, word)\n",
    "        probs = model(input_vector)\n",
    "        prob_list.append(probs)\n",
    "        max_val, predicted_label = torch.max(probs, 1)\n",
    "        predicted_label = predicted_label.data[0]\n",
    "        prev_label = tag_embeddings[predicted_label]\n",
    "        output_labels[i] = predicted_label\n",
    "#     print output_labels\n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_inference(model, sentence, word_embeddings, tag_embeddings, NUM_LABELS):\n",
    "        dp = np.zeros((NUM_LABELS, len(sentence)+1))\n",
    "        back_pointers = np.zeros((NUM_LABELS, len(sentence)))\n",
    "        dp[0][0] = 1\n",
    "        for i in range(len(sentence)):\n",
    "            word_table = np.zeros((NUM_LABELS, NUM_LABELS))\n",
    "            if i == 0:\n",
    "                input_vector = get_input_vector(word_embeddings, tag_embeddings[NUM_LABELS], sentence[i])\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "                word_table[:,0] = np.multiply(dp[0, 0], probs)\n",
    "                dp[:,i+1] = word_table[:,0]\n",
    "                back_pointers[:,i] = 128\n",
    "                continue\n",
    "            for j in range(NUM_LABELS):\n",
    "                input_vector = get_input_vector(word_embeddings, tag_embeddings[j], sentence[i])\n",
    "                probs = model(input_vector)\n",
    "                probs = probs.data.numpy()\n",
    "                word_table[:,j] = np.multiply(dp[j, i], probs)\n",
    "            dp[:,i+1] = word_table.max(1)\n",
    "            for k in range(NUM_LABELS):\n",
    "                for index, element in enumerate(word_table[k]):\n",
    "                    if element == dp[k, i+1]:\n",
    "                        back_pointers[k, i] = index\n",
    "        output_labels = np.zeros(len(sentence), dtype = np.int)\n",
    "        label_index = len(sentence) - 1\n",
    "        max_val = dp[:, len(sentence)].max()\n",
    "        for index, element in enumerate(dp[:, len(sentence)]):\n",
    "            if element == max_val:\n",
    "                output_labels[label_index] = index\n",
    "                break\n",
    "        for i in range(len(sentence)-1, 0, -1): #18 to 1\n",
    "            row = back_pointers[output_labels[label_index], i]\n",
    "            label_index -= 1\n",
    "            output_labels[label_index] = row\n",
    "        return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results  66.38 66.13 66.25\n"
     ]
    }
   ],
   "source": [
    "predictions_valid = [ map(lambda t: idx2label[t],\n",
    "    viterbi_inference(model, x, word_embeddings, tag_embeddings, NUM_LABELS)) for x in valid_lex]\n",
    "# print \"predictions \", predictions_valid[0]\n",
    "groundtruth_valid = [ map(lambda t: idx2label[t], y) for y in valid_y ]\n",
    "\n",
    "# print \"groundtruth \", groundtruth_valid[0]\n",
    "words_valid = [ map(lambda t: idx2word[t], w) for w in valid_lex ]\n",
    "valid_precision, valid_recall, valid_f1score = conlleval(predictions_valid, groundtruth_valid, words_valid)\n",
    "print \"Validation results \", valid_precision, valid_recall, valid_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results  65.09 63.24 64.15\n"
     ]
    }
   ],
   "source": [
    "predictions_test = [ map(lambda t: idx2label[t],\n",
    " viterbi_inference(model, x, word_embeddings, tag_embeddings, NUM_LABELS)) for x in test_lex]\n",
    "# print \"predictions \", predictions_test[0]\n",
    "groundtruth_test = [ map(lambda t: idx2label[t], y) for y in test_y ]\n",
    "\n",
    "# print \"groundtruth \", groundtruth_test[0]\n",
    "words_test = [ map(lambda t: idx2word[t], w) for w in test_lex ]\n",
    "test_precision, test_recall, test_f1score = conlleval(predictions_test, groundtruth_test, words_test)\n",
    "print \"Test Results \", test_precision, test_recall, test_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(tag_embeddings)\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
